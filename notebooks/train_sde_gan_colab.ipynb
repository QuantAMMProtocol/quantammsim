{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SDE-GAN Training on GPU\n\nTrain a Neural SDE-GAN for synthetic multi-asset price path generation.\n\n**Runtime**: Go to Runtime > Change runtime type > **T4 GPU** (free tier) or A100 (Colab Pro).\n\n**Configs** (auto-detected by GPU):\n- **T4/L4**: Kidger-scale model (hidden=16, depth=1) + WGAN-GP + large batch.\n- **A100+**: Larger model (hidden=64, depth=2) + WGAN-GP.\n\n**Data**: You need minute-resolution parquet files (`ETH_USD.parquet`, etc.).\nThree options (pick one in the data cell below):\n1. **Google Drive** (recommended): upload parquets to Drive once, reuse across sessions\n2. **Direct upload**: use Colab file upload widget (~623 MB, a few minutes)\n3. **Download from Binance**: uses `update_historic_data` pipeline (~15-30 min)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU\nimport subprocess\nresult = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\nprint(result.stdout)\nif 'A100' in result.stdout or 'V100' in result.stdout or 'H100' in result.stdout:\n    GPU_TIER = 'A100'\n    print('High-end GPU detected — using large model + WGAN-GP config')\nelse:\n    GPU_TIER = 'T4'  # T4, L4, or unknown\n    print('T4/L4 detected — using Kidger-scale config + WGAN-GP')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install deps\n!pip install -q jax[cuda12] equinox diffrax optax\n!pip install -q dask pandas pyarrow"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone repo\nimport os\nif not os.path.exists('/content/quantammsim'):\n    !git clone -b synthetic-price-gen https://github.com/QuantAMMProtocol/quantammsim.git /content/quantammsim\nos.chdir('/content/quantammsim')\n!pip install -q -e ."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify JAX sees GPU\n",
    "import jax\n",
    "print(f'JAX devices: {jax.devices()}')\n",
    "assert any(d.platform == 'gpu' for d in jax.devices()), 'No GPU! Change runtime type.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from quantammsim.synthetic.sde_gan import (\n",
    "    train_sde_gan, generate_paths, compute_daily_log_prices,\n",
    ")\n",
    "from quantammsim.utils.data_processing.historic_data_utils import get_historic_parquet_data"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================\n# DATA: pick ONE method, comment out the others\n# =============================================================\ntokens = ['ETH', 'BTC', 'USDC', 'PAXG']\nDATA_DIR = 'quantammsim/data'\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# --- Option 1: Google Drive (recommended) --------------------\n# Upload parquets to Drive once (e.g. My Drive/quantammsim_data/),\n# then every session just mounts and copies.\nfrom google.colab import drive\ndrive.mount('/content/drive')\nDRIVE_DATA = '/content/drive/My Drive/quantammsim_data'  # adjust path\nfor t in tokens:\n    src = f'{DRIVE_DATA}/{t}_USD.parquet'\n    dst = f'{DATA_DIR}/{t}_USD.parquet'\n    if not os.path.exists(dst):\n        !cp \"{src}\" \"{dst}\"\n        print(f'Copied {t}_USD.parquet from Drive')\n    else:\n        print(f'{t}_USD.parquet already present')\n\n# --- Option 2: Direct upload ---------------------------------\n# Uncomment below, comment out Option 1 above.\n#\n# from google.colab import files\n# print('Upload: ETH_USD.parquet, BTC_USD.parquet, USDC_USD.parquet, PAXG_USD.parquet')\n# uploaded = files.upload()\n# for name, data in uploaded.items():\n#     with open(f'{DATA_DIR}/{name}', 'wb') as f:\n#         f.write(data)\n#     print(f'Saved {name} ({len(data)/1e6:.1f} MB)')\n\n# --- Option 3: Download from Binance Vision ------------------\n# Slow (~15-30 min) but fully automated. Uncomment below,\n# comment out Options 1 & 2, and add to the pip install cell:\n#   !pip install -q binance-historical-data Historic-Crypto bidask\n#\n# from quantammsim.utils.data_processing.historic_data_utils import update_historic_data\n# data_dir_str = DATA_DIR + '/'\n# for token in tokens:\n#     if os.path.exists(f'{DATA_DIR}/{token}_USD.parquet'):\n#         print(f'{token}_USD.parquet exists, skipping')\n#         continue\n#     print(f'Downloading {token}...')\n#     update_historic_data(token, data_dir_str)\n#     combined = f'{DATA_DIR}/combined_data/{token}_USD.parquet'\n#     if os.path.exists(combined):\n#         os.rename(combined, f'{DATA_DIR}/{token}_USD.parquet')\n#     print(f'{token} done')\n\n# Verify\nprint('\\nParquet files:')\nfor t in tokens:\n    p = f'{DATA_DIR}/{t}_USD.parquet'\n    if os.path.exists(p):\n        sz = os.path.getsize(p) / 1e6\n        print(f'  {t}_USD.parquet ({sz:.1f} MB)')\n    else:\n        print(f'  {t}_USD.parquet MISSING!')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tokens = ['ETH', 'BTC', 'USDC', 'PAXG']\n",
    "data_root = 'quantammsim/data'\n",
    "price_df = get_historic_parquet_data(tokens, cols=['close'], root=data_root)\n",
    "close_cols = [f'close_{t}' for t in tokens]\n",
    "minute_prices = price_df[close_cols].values.astype(np.float64)\n",
    "valid_mask = ~np.any(np.isnan(minute_prices), axis=1)\n",
    "first_valid = np.argmax(valid_mask)\n",
    "last_valid = len(valid_mask) - np.argmax(valid_mask[::-1])\n",
    "minute_prices = minute_prices[first_valid:last_valid]\n",
    "n_assets = len(tokens)\n",
    "minute_prices_jnp = jnp.array(minute_prices)\n",
    "daily_log = compute_daily_log_prices(minute_prices_jnp)\n",
    "n_days = daily_log.shape[0]\n",
    "\n",
    "real_returns = jnp.diff(daily_log, axis=0)\n",
    "real_drift = jnp.mean(real_returns, axis=0)\n",
    "real_vol = jnp.std(real_returns, axis=0)\n",
    "real_corr = jnp.corrcoef(real_returns.T)\n",
    "\n",
    "print(f'Data: {n_days} days, {n_assets} assets')\n",
    "for i, t in enumerate(tokens):\n",
    "    print(f'  {t}: drift={float(real_drift[i]):.6f}/day, vol={float(real_vol[i]):.6f}/day')\n",
    "print(f'\\nCorrelations:')\n",
    "for i in range(n_assets):\n",
    "    for j in range(i+1, n_assets):\n",
    "        print(f'  {tokens[i]}-{tokens[j]}: {float(real_corr[i,j]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Config\n\nAuto-detected by GPU. T4 gets Kidger-scale (weight clipping), L4/A100+ gets larger model (WGAN-GP)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if GPU_TIER == 'A100':\n    # A100 / V100 / H100 — larger model stabilized with WGAN-GP\n    CONFIG = dict(\n        hidden_size=64, width_size=64, depth=2,\n        noise_size=8, initial_noise_size=8,\n        batch_size=1024, window_len=50,\n        n_steps=20000,\n        generator_lr=2e-5, discriminator_lr=1e-4,\n        drift_lambda=1.0,\n        gp_lambda=10.0,  # WGAN-GP instead of weight clipping\n        use_reversible_heun=True,\n    )\nelse:\n    # T4 / L4 — Kidger-scale model + WGAN-GP\n    CONFIG = dict(\n        hidden_size=16, width_size=16, depth=1,\n        noise_size=3, initial_noise_size=5,\n        batch_size=1024, window_len=50,\n        n_steps=10000,\n        generator_lr=2e-5, discriminator_lr=1e-4,\n        drift_lambda=1.0,\n        gp_lambda=10.0,  # WGAN-GP instead of weight clipping\n        use_reversible_heun=True,\n    )\n\nprint(f'Config ({GPU_TIER}):')\nfor k, v in CONFIG.items():\n    print(f'  {k}: {v}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport equinox as eqx\n\nSAVE_DIR = '/content/drive/My Drive/quantammsim_data'\n\ndef checkpoint(gen, vs, step):\n    \"\"\"Save model to Drive during training (survives disconnects).\"\"\"\n    eqx.tree_serialise_leaves(f'{SAVE_DIR}/generator.eqx', gen)\n    np.save(f'{SAVE_DIR}/vol_scale.npy', np.array(vs))\n    print(f'  [checkpoint] saved at step {step}')\n\nkey = jax.random.PRNGKey(42)\nt0 = time.time()\n\ngenerator, vol_scale, history = train_sde_gan(\n    minute_prices_jnp, n_assets=n_assets, key=key,\n    verbose=True, checkpoint_fn=checkpoint, checkpoint_every=2000,\n    **CONFIG,\n)\n\nelapsed = time.time() - t0\nprint(f'\\nDone in {elapsed:.0f}s ({elapsed/CONFIG[\"n_steps\"]*1000:.1f}ms/step)')\n\n# Final save\ncheckpoint(generator, vol_scale, CONFIG['n_steps'])\nprint('Training complete, model saved to Drive.')"
  },
  {
   "cell_type": "code",
   "source": "# Load saved model (use this instead of retraining in a new session)\n# Uncomment this cell and skip the Train cell above.\n#\n# import equinox as eqx\n# from quantammsim.synthetic.sde_gan import Generator\n# SAVE_DIR = '/content/drive/My Drive/quantammsim_data'\n# skeleton = Generator(\n#     data_size=n_assets,\n#     initial_noise_size=CONFIG['initial_noise_size'],\n#     noise_size=CONFIG['noise_size'],\n#     hidden_size=CONFIG['hidden_size'],\n#     width_size=CONFIG['width_size'],\n#     depth=CONFIG['depth'],\n#     use_reversible_heun=CONFIG.get('use_reversible_heun', False),\n#     key=jax.random.PRNGKey(0),\n# )\n# generator = eqx.tree_deserialise_leaves(f'{SAVE_DIR}/generator.eqx', skeleton)\n# vol_scale = jnp.array(np.load(f'{SAVE_DIR}/vol_scale.npy'))\n# print(f'Loaded model from {SAVE_DIR}/')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = daily_log[0]\n",
    "key_eval = jax.random.PRNGKey(99)\n",
    "N_PATHS = 2000\n",
    "\n",
    "for horizon in [10, 30, 50, 100, 200]:\n",
    "    paths = generate_paths(generator, vol_scale, y0,\n",
    "                           n_days=horizon, n_paths=N_PATHS, key=key_eval)\n",
    "    y0_bc = jnp.broadcast_to(y0[:, None], (n_assets, N_PATHS))[None, ...]\n",
    "    full = jnp.concatenate([y0_bc, paths], axis=0)\n",
    "    returns = jnp.diff(full, axis=0)\n",
    "    drift = jnp.mean(returns, axis=(0, 2))\n",
    "    vol = jnp.mean(jnp.std(returns, axis=2), axis=0)\n",
    "\n",
    "    flat_ret = returns.transpose(0, 2, 1).reshape(-1, n_assets)\n",
    "    gen_corr = jnp.corrcoef(flat_ret.T)\n",
    "\n",
    "    print(f'\\n--- {horizon}-day paths ---')\n",
    "    for i, t in enumerate(tokens):\n",
    "        rd, rv = float(real_drift[i]), float(real_vol[i])\n",
    "        d, v = float(drift[i]), float(vol[i])\n",
    "        dr = d / rd if abs(rd) > 1e-8 else float('inf')\n",
    "        print(f'  {t}: drift={d:.6f} ({dr:.1f}x), vol={v:.6f} ({v/rv:.2f}x)')\n",
    "    print(f'  Correlations (real -> gen):')\n",
    "    for i in range(n_assets):\n",
    "        for j in range(i+1, n_assets):\n",
    "            print(f'    {tokens[i]}-{tokens[j]}: {float(real_corr[i,j]):.3f} -> {float(gen_corr[i,j]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Sweep (optional)\n",
    "\n",
    "Run this cell to sweep drift_lambda and find the optimal value for your GPU config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: sweep drift_lambda\n",
    "SWEEP = False  # Set to True to run\n",
    "\n",
    "if SWEEP:\n",
    "    sweep_results = {}\n",
    "    for lam in [0.0, 0.1, 0.5, 1.0, 2.0]:\n",
    "        print(f'\\n{\"=\"*50}')\n",
    "        print(f'drift_lambda = {lam}')\n",
    "        print(f'{\"=\"*50}')\n",
    "        cfg = {**CONFIG, 'drift_lambda': lam}\n",
    "        key_s = jax.random.PRNGKey(42)\n",
    "        gen_s, vs_s, hist_s = train_sde_gan(\n",
    "            minute_prices_jnp, n_assets=n_assets, key=key_s,\n",
    "            verbose=True, **cfg,\n",
    "        )\n",
    "        # Quick 50d eval\n",
    "        paths_s = generate_paths(gen_s, vs_s, y0, n_days=50, n_paths=1000, key=key_eval)\n",
    "        y0_bc_s = jnp.broadcast_to(y0[:, None], (n_assets, 1000))[None, ...]\n",
    "        full_s = jnp.concatenate([y0_bc_s, paths_s], axis=0)\n",
    "        ret_s = jnp.diff(full_s, axis=0)\n",
    "        d_s = jnp.mean(ret_s, axis=(0, 2))\n",
    "        v_s = jnp.mean(jnp.std(ret_s, axis=2), axis=0)\n",
    "        flat_s = ret_s.transpose(0, 2, 1).reshape(-1, n_assets)\n",
    "        gc_s = jnp.corrcoef(flat_s.T)\n",
    "        sweep_results[lam] = {\n",
    "            'drift_ratios': {t: float(d_s[i])/float(real_drift[i]) if abs(float(real_drift[i])) > 1e-8 else float('inf') for i, t in enumerate(tokens)},\n",
    "            'vol_ratios': {t: float(v_s[i])/float(real_vol[i]) for i, t in enumerate(tokens)},\n",
    "            'eth_btc_corr': float(gc_s[0, 1]),\n",
    "        }\n",
    "\n",
    "    # Summary table\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print('SWEEP SUMMARY (50-day horizon)')\n",
    "    print(f'{\"=\"*60}')\n",
    "    print(f'{\"lambda\":>8} | {\"ETH drift\":>10} {\"BTC drift\":>10} {\"ETH-BTC corr\":>13}')\n",
    "    print('-' * 50)\n",
    "    for lam, r in sweep_results.items():\n",
    "        ed = r['drift_ratios'].get('ETH', 0)\n",
    "        bd = r['drift_ratios'].get('BTC', 0)\n",
    "        ec = r['eth_btc_corr']\n",
    "        print(f'{lam:>8.1f} | {ed:>9.1f}x {bd:>9.1f}x {ec:>12.3f} (real: 0.813)')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}